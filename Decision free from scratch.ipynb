{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8be4d0f",
   "metadata": {},
   "source": [
    "Decision Tree from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e49ef36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdea3e02",
   "metadata": {},
   "source": [
    "Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23dcf48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of row:  8124\n",
      "num of col:  23\n",
      "num of row after removing:  5644\n"
     ]
    }
   ],
   "source": [
    "col_names = ['edibility','cap-shape', \"cap-surface\", \"cap-color\", \"bruises\", \n",
    "             \"odor\", \"gill-attachment\", \"gill-spacing\", \"gill-size\", \"gill-color\", \n",
    "             \"stalk-shape\",\"stalk-root\", \"stalk-surface-above-ring\", \"stalk-surface-below-ring\",\n",
    "               \"stalk-color-above-ring\", \"stalk-color-below-ring\", \"veil-type\",\n",
    "                \"veil-color\", \"ring-number\", \"ring-type\", \"spore-print-color\",\n",
    "                 \"population\", \"habitat\" ]\n",
    "\n",
    "#read data\n",
    "mushroom_df = pd.read_table('agaricus-lepiota.data', sep=',', header = None, names = col_names)\n",
    "\n",
    "#number of rows\n",
    "print(\"num of row: \" , len(mushroom_df))\n",
    "#number of columns\n",
    "print(\"num of col: \", len(mushroom_df.columns))\n",
    "\n",
    "#check the index number where \"?\" exist \n",
    "missing_rows_num = []\n",
    "for col in mushroom_df:\n",
    "    count = -1\n",
    "    for rows in mushroom_df[col]:\n",
    "        count += 1\n",
    "        if rows == \"?\":\n",
    "            missing_rows_num.append(count)\n",
    "\n",
    "missing_rows_num2 = np.unique(missing_rows_num)\n",
    "\n",
    "#remove missing value rows\n",
    "mushroom_df2 = mushroom_df\n",
    "for i in missing_rows_num2:\n",
    "    mushroom_df2 = mushroom_df2.drop(i)\n",
    "\n",
    "#num of rows after remove all missing rows\n",
    "print(\"num of row after removing: \", len(mushroom_df2)), \n",
    "\n",
    "#split a dataset into 70% for train, 30% for test (pandas.sample is used)\n",
    "train_df =mushroom_df2.sample(frac=0.7,random_state=123)\n",
    "test_df =mushroom_df2.drop(train_df.index)\n",
    "\n",
    "\n",
    "Ycol_name = \"edibility\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32f432a",
   "metadata": {},
   "source": [
    "Implement DecisionTree algorithm with a train procedure - using information gain criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "943c178d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'odor': {'a': 'e',\n",
       "  'c': 'p',\n",
       "  'f': 'p',\n",
       "  'l': 'e',\n",
       "  'm': 'p',\n",
       "  'n': {'spore-print-color': {'k': 'e',\n",
       "    'n': 'e',\n",
       "    'r': 'p',\n",
       "    'w': {'cap-color': {'c': 'e',\n",
       "      'g': 'e',\n",
       "      'n': 'e',\n",
       "      'p': 'e',\n",
       "      'w': 'p',\n",
       "      'y': 'p'}}}},\n",
       "  'p': 'p'}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculating entropy\n",
    "def calc_entropy(train_df, col_name):\n",
    "    Y_vale_list = np.unique(train_df[Ycol_name]) \n",
    "    total_entropy = 0\n",
    "    \n",
    "    for i in Y_vale_list:\n",
    "        num_of_i_row = len(train_df[train_df[col_name] == i])\n",
    "        #entropy of i\n",
    "        if num_of_i_row != 0:\n",
    "            i_Entropy = ((num_of_i_row/len(train_df)) * math.log2(num_of_i_row/len(train_df)))\n",
    "            total_entropy += i_Entropy\n",
    "    return -total_entropy\n",
    "\n",
    "\n",
    "#calculate IG value\n",
    "def calc_IG (train_df, attribute_name, Ycol_name):\n",
    "    #list of values in the feature \n",
    "    value_in_attribute = np.unique(train_df[attribute_name])\n",
    "    total_X_entropy = 0\n",
    "\n",
    "    for i in value_in_attribute:\n",
    "        #split df to sub df which only contains specific value\n",
    "        splited_df = train_df[train_df[attribute_name] == i]\n",
    "        i_value_entropy = calc_entropy(splited_df, Ycol_name)\n",
    "        i_entropy = (len(splited_df)/len(train_df)) * i_value_entropy\n",
    "        total_X_entropy += i_entropy\n",
    "\n",
    "    Y_entropy = calc_entropy(train_df, Ycol_name)\n",
    "    #return IG value\n",
    "    return (Y_entropy - total_X_entropy)\n",
    "\n",
    "\n",
    "def find_highest_IG(train_df, Ycol_name):\n",
    "    #minimum IG value is 0\n",
    "    max_IG = -1\n",
    "    best_node = \"error\"\n",
    "    Xs_df = train_df.drop(labels=Ycol_name, axis = 1)\n",
    "    #compare each attributes' IG value \n",
    "    for attribute in Xs_df:\n",
    "        IG_value = calc_IG(train_df, attribute, Ycol_name)\n",
    "        if max_IG < IG_value:\n",
    "            max_IG = IG_value\n",
    "            best_node = attribute\n",
    "    return best_node\n",
    "\n",
    "\n",
    "def decision_tree(train_df, Ycol_name, tree = None):\n",
    "    #find (root) node\n",
    "    node = find_highest_IG(train_df, Ycol_name)\n",
    "    value_in_attribute = np.unique(train_df[node])\n",
    "    \n",
    "    #creating dic to print tree\n",
    "    if tree is None:\n",
    "        tree= {}\n",
    "        tree[node] = {}\n",
    "\n",
    "    #build a tree\n",
    "    for i in value_in_attribute:\n",
    "        splited_df = train_df[train_df[node]== i]\n",
    "        impurity_check = np.unique(splited_df[Ycol_name],return_counts = True)\n",
    "        #check the splited node has pure leaf or not\n",
    "        if len(impurity_check[0]) == 1:\n",
    "            leaf_node = impurity_check[0][0]\n",
    "            tree[node][i] = leaf_node\n",
    "        else:\n",
    "            tree[node][i] = decision_tree(splited_df, Ycol_name)\n",
    "    return tree\n",
    "\n",
    "\n",
    "\n",
    "decision_tree(train_df, Ycol_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab12000d",
   "metadata": {},
   "source": [
    "Implement decisoin tree depth control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72f5d8b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'odor': {'a': 'e',\n",
       "  'c': 'p',\n",
       "  'f': 'p',\n",
       "  'l': 'e',\n",
       "  'm': 'p',\n",
       "  'n': {'spore-print-color': {'k': 'e', 'n': 'e', 'r': 'p', 'w': 'e'}},\n",
       "  'p': 'p'}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def stopping_depth(depth):\n",
    "    depth_number = depth\n",
    "    count = 0\n",
    "    return decision_tree2(train_df, Ycol_name, depth_number, count)\n",
    "\n",
    "def decision_tree2(train_df, Ycol_name, depth_number, count, tree = None):\n",
    "    #find (root) node\n",
    "    node = find_highest_IG(train_df, Ycol_name)\n",
    "    value_in_attribute = np.unique(train_df[node])\n",
    "    \n",
    "    #count the depth\n",
    "    if tree is None:\n",
    "        count += 1\n",
    "        tree= {}\n",
    "        tree[node] = {}\n",
    "\n",
    "    for i in value_in_attribute:\n",
    "        splited_df = train_df[train_df[node]== i]\n",
    "        impurity_check = np.unique(splited_df[Ycol_name],return_counts = True)\n",
    "        \n",
    "        #number of e and p\n",
    "        each_value_num = impurity_check[1].tolist()\n",
    "        #values of edibility \n",
    "        each_value = impurity_check[0].tolist()\n",
    "        \n",
    "        #check the splited node has pure leaf or not\n",
    "        if len(impurity_check[0]) == 1:\n",
    "            leaf_node = impurity_check[0][0]\n",
    "            tree[node][i] = leaf_node\n",
    "        elif count == depth_number:\n",
    "            #majority rule applied to choose the leaf node\n",
    "            if each_value_num[1] < each_value_num[0]:\n",
    "                tree[node][i] = each_value[0]\n",
    "            else:\n",
    "                tree[node][i] = each_value[1]\n",
    "        else:\n",
    "            tree[node][i] = decision_tree2(splited_df, Ycol_name, depth_number, count)\n",
    "    return tree\n",
    "\n",
    "#tree which stopped at depth 2\n",
    "stopping_depth(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34c08a0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'odor': {'a': 'e',\n",
       "  'c': 'p',\n",
       "  'f': 'p',\n",
       "  'l': 'e',\n",
       "  'm': 'p',\n",
       "  'n': {'spore-print-color': {'k': 'e',\n",
       "    'n': 'e',\n",
       "    'r': 'p',\n",
       "    'w': {'cap-color': {'c': 'e',\n",
       "      'g': 'e',\n",
       "      'n': 'e',\n",
       "      'p': 'e',\n",
       "      'w': 'p',\n",
       "      'y': 'p'}}}},\n",
       "  'p': 'p'}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tree which stopped at depth 3\n",
    "stopping_depth(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75569181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'odor': {'a': 'e',\n",
       "  'c': 'p',\n",
       "  'f': 'p',\n",
       "  'l': 'e',\n",
       "  'm': 'p',\n",
       "  'n': {'spore-print-color': {'k': 'e',\n",
       "    'n': 'e',\n",
       "    'r': 'p',\n",
       "    'w': {'cap-color': {'c': 'e',\n",
       "      'g': 'e',\n",
       "      'n': 'e',\n",
       "      'p': 'e',\n",
       "      'w': 'p',\n",
       "      'y': 'p'}}}},\n",
       "  'p': 'p'}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tree which stopped at depth 4\n",
    "stopping_depth(4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a36a56",
   "metadata": {},
   "source": [
    "Implement a test procedure that takes new data and the trained model and returns a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6779a5b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_TP(tree, splitted_df, Ycol_name):\n",
    "    key_list = list(tree.keys())\n",
    "    node = key_list[0]\n",
    "    node_value = splitted_df[node][0]\n",
    "    predict_Y = tree[node][node_value]\n",
    "    actual_Y = splitted_df[Ycol_name][0]\n",
    "    if predict_Y == actual_Y:\n",
    "        return 1\n",
    "    elif type(predict_Y) is not dict:\n",
    "        return 0\n",
    "    else:\n",
    "        return check_TP(predict_Y, splitted_df, Ycol_name)\n",
    "    \n",
    "    \n",
    "def test(tree, test_df, Ycol_name):\n",
    "    TP= 0\n",
    "    for row in test_df.to_numpy():\n",
    "        splitted_df = pd.DataFrame([row])\n",
    "        splitted_df.columns = col_names\n",
    "        #result is 1 when it is TP, 0 otherwise\n",
    "        result = check_TP(tree, splitted_df, Ycol_name)\n",
    "        TP += result\n",
    "    #calculate accuracy percentage\n",
    "    return (TP/(len(test_df))) * 100\n",
    "\n",
    "tree = decision_tree(train_df, Ycol_name)\n",
    "test(tree, test_df, Ycol_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09563ed2",
   "metadata": {},
   "source": [
    "**test evaluation:** I used accuracy to test my decision tree model performance. I calculate the number of correct (TP) values. Based on the number of TP in the testing is used in the accuracy formula: TP/total. The test(tree, test_df, Ycol_name) function results with 100% accuracy, which means my model predicts well with the mushroom data frame. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2efbdac1",
   "metadata": {},
   "source": [
    "Explain whether the evaluation method can indicate whether your tree is over- or underfitting\n",
    "\n",
    "Overfitting is when a model has 100% accuracy with the train data while it has poor accuracy with the test data. This indicates that our model did not capture the relationship which are in test data. In this case, we can prune the tree to make it more general. On the other hand, underfitting is when a model has poor accuracy with both the train and test data. This indicates that our model fails to capture relationship which are in both data sets. In this case, we can increase the depth of the tree in order to increase the accuracy of both data sets. When my training decision tree only has depth 2, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f002f409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99.88186650915534"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#accuracy when tree depth is 2\n",
    "test(stopping_depth(2), test_df, Ycol_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7c959dfb",
   "metadata": {},
   "source": [
    "it has a 99.9% accuracy with the test data.  \n",
    "This indicates that my evaluation method to generate a tree is not underfitting and perfectly fits. However, in general, if a trained model has 100% accuracy with the train data, it would have lower accuracy since the tree is overfitting. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "edc5e595d7ef337b338ddd9c3b7d927795bc30a52fb6b7c2dd8276fdae2f0a37"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
