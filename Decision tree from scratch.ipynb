{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8be4d0f",
   "metadata": {},
   "source": [
    "Decision Tree from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e49ef36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdea3e02",
   "metadata": {},
   "source": [
    "Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "23dcf48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of row:  8124\n",
      "num of col:  23\n",
      "num of row after removing:  5644\n"
     ]
    }
   ],
   "source": [
    "col_names = ['edibility','cap-shape', \"cap-surface\", \"cap-color\", \"bruises\", \n",
    "             \"odor\", \"gill-attachment\", \"gill-spacing\", \"gill-size\", \"gill-color\", \n",
    "             \"stalk-shape\",\"stalk-root\", \"stalk-surface-above-ring\", \"stalk-surface-below-ring\",\n",
    "               \"stalk-color-above-ring\", \"stalk-color-below-ring\", \"veil-type\",\n",
    "                \"veil-color\", \"ring-number\", \"ring-type\", \"spore-print-color\",\n",
    "                 \"population\", \"habitat\" ]\n",
    "\n",
    "#read data\n",
    "mushroom_df = pd.read_table('agaricus-lepiota.data', sep=',', header = None, names = col_names)\n",
    "\n",
    "#number of rows\n",
    "print(\"num of row: \" , len(mushroom_df))\n",
    "#number of columns\n",
    "print(\"num of col: \", len(mushroom_df.columns))\n",
    "\n",
    "#check the index number where \"?\" exist \n",
    "missing_rows_num = []\n",
    "for col in mushroom_df:\n",
    "    count = -1\n",
    "    for rows in mushroom_df[col]:\n",
    "        count += 1\n",
    "        if rows == \"?\":\n",
    "            missing_rows_num.append(count)\n",
    "\n",
    "missing_rows_num2 = np.unique(missing_rows_num)\n",
    "\n",
    "#remove missing value rows\n",
    "mushroom_df2 = mushroom_df\n",
    "for i in missing_rows_num2:\n",
    "    mushroom_df2 = mushroom_df2.drop(i)\n",
    "\n",
    "#num of rows after remove all missing rows\n",
    "print(\"num of row after removing: \", len(mushroom_df2)), \n",
    "\n",
    "#split a dataset into 70% for train, 30% for test (pandas.sample is used)\n",
    "train_df =mushroom_df2.sample(frac=0.7,random_state=123)\n",
    "test_df =mushroom_df2.drop(train_df.index)\n",
    "\n",
    "\n",
    "Ycol_name = \"edibility\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32f432a",
   "metadata": {},
   "source": [
    "Implement DecisionTree algorithm with a train procedure - using information gain criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "943c178d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'odor': {'a': 'e',\n",
       "  'c': 'p',\n",
       "  'f': 'p',\n",
       "  'l': 'e',\n",
       "  'm': 'p',\n",
       "  'n': {'spore-print-color': {'k': 'e',\n",
       "    'n': 'e',\n",
       "    'r': 'p',\n",
       "    'w': {'cap-color': {'c': 'e',\n",
       "      'g': 'e',\n",
       "      'n': 'e',\n",
       "      'p': 'e',\n",
       "      'w': 'p',\n",
       "      'y': 'p'}}}},\n",
       "  'p': 'p'}}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculating entropy\n",
    "def calc_entropy(train_df, col_name):\n",
    "    Y_vale_list = np.unique(train_df[Ycol_name]) \n",
    "    total_entropy = 0\n",
    "    \n",
    "    for i in Y_vale_list:\n",
    "        num_of_i_row = len(train_df[train_df[col_name] == i])\n",
    "        #entropy of i\n",
    "        if num_of_i_row != 0:\n",
    "            i_Entropy = ((num_of_i_row/len(train_df)) * math.log2(num_of_i_row/len(train_df)))\n",
    "            total_entropy += i_Entropy\n",
    "    return -total_entropy\n",
    "\n",
    "\n",
    "#calculate IG value\n",
    "def calc_IG (train_df, attribute_name, Ycol_name):\n",
    "    #list of values in the feature \n",
    "    value_in_attribute = np.unique(train_df[attribute_name])\n",
    "    total_X_entropy = 0\n",
    "\n",
    "    for i in value_in_attribute:\n",
    "        #split df to sub df which only contains specific value\n",
    "        splited_df = train_df[train_df[attribute_name] == i]\n",
    "        i_value_entropy = calc_entropy(splited_df, Ycol_name)\n",
    "        i_entropy = (len(splited_df)/len(train_df)) * i_value_entropy\n",
    "        total_X_entropy += i_entropy\n",
    "\n",
    "    Y_entropy = calc_entropy(train_df, Ycol_name)\n",
    "    #return IG value\n",
    "    return (Y_entropy - total_X_entropy)\n",
    "\n",
    "\n",
    "def find_highest_IG(train_df, Ycol_name):\n",
    "    #minimum IG value is 0\n",
    "    max_IG = -1\n",
    "    best_node = \"error\"\n",
    "    Xs_df = train_df.drop(labels=Ycol_name, axis = 1)\n",
    "    #compare each attributes' IG value \n",
    "    for attribute in Xs_df:\n",
    "        IG_value = calc_IG(train_df, attribute, Ycol_name)\n",
    "        if max_IG < IG_value:\n",
    "            max_IG = IG_value\n",
    "            best_node = attribute\n",
    "    return best_node\n",
    "\n",
    "\n",
    "def decision_tree(train_df, Ycol_name, tree = None):\n",
    "    #find (root) node\n",
    "    node = find_highest_IG(train_df, Ycol_name)\n",
    "    value_in_attribute = np.unique(train_df[node])\n",
    "    \n",
    "    #creating dic to print tree\n",
    "    if tree is None:\n",
    "        tree= {}\n",
    "        tree[node] = {}\n",
    "\n",
    "    #build a tree\n",
    "    for i in value_in_attribute:\n",
    "        splited_df = train_df[train_df[node]== i]\n",
    "        impurity_check = np.unique(splited_df[Ycol_name],return_counts = True)\n",
    "        #check the splited node has pure leaf or not\n",
    "        if len(impurity_check[0]) == 1:\n",
    "            leaf_node = impurity_check[0][0]\n",
    "            tree[node][i] = leaf_node\n",
    "        else:\n",
    "            tree[node][i] = decision_tree(splited_df, Ycol_name)\n",
    "    return tree\n",
    "\n",
    "\n",
    "\n",
    "decision_tree(train_df, Ycol_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab12000d",
   "metadata": {},
   "source": [
    "Implement decisoin tree depth control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "72f5d8b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'odor': {'a': 'e',\n",
       "  'c': 'p',\n",
       "  'f': 'p',\n",
       "  'l': 'e',\n",
       "  'm': 'p',\n",
       "  'n': {'spore-print-color': {'k': 'e', 'n': 'e', 'r': 'p', 'w': 'e'}},\n",
       "  'p': 'p'}}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def stopping_depth(depth):\n",
    "    depth_number = depth\n",
    "    count = 0\n",
    "    return decision_tree2(train_df, Ycol_name, depth_number, count)\n",
    "\n",
    "def decision_tree2(train_df, Ycol_name, depth_number, count, tree = None):\n",
    "    #find (root) node\n",
    "    node = find_highest_IG(train_df, Ycol_name)\n",
    "    value_in_attribute = np.unique(train_df[node])\n",
    "    \n",
    "    #count the depth\n",
    "    if tree is None:\n",
    "        count += 1\n",
    "        tree= {}\n",
    "        tree[node] = {}\n",
    "\n",
    "    for i in value_in_attribute:\n",
    "        splited_df = train_df[train_df[node]== i]\n",
    "        impurity_check = np.unique(splited_df[Ycol_name],return_counts = True)\n",
    "        \n",
    "        #number of e and p\n",
    "        each_value_num = impurity_check[1].tolist()\n",
    "        #values of edibility \n",
    "        each_value = impurity_check[0].tolist()\n",
    "        \n",
    "        #check the splited node has pure leaf or not\n",
    "        if len(impurity_check[0]) == 1:\n",
    "            leaf_node = impurity_check[0][0]\n",
    "            tree[node][i] = leaf_node\n",
    "        elif count == depth_number:\n",
    "            #majority rule applied to choose the leaf node\n",
    "            if each_value_num[1] < each_value_num[0]:\n",
    "                tree[node][i] = each_value[0]\n",
    "            else:\n",
    "                tree[node][i] = each_value[1]\n",
    "        else:\n",
    "            tree[node][i] = decision_tree2(splited_df, Ycol_name, depth_number, count)\n",
    "    return tree\n",
    "\n",
    "#tree which stopped at depth 2\n",
    "stopping_depth(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "34c08a0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'odor': {'a': 'e',\n",
       "  'c': 'p',\n",
       "  'f': 'p',\n",
       "  'l': 'e',\n",
       "  'm': 'p',\n",
       "  'n': {'spore-print-color': {'k': 'e',\n",
       "    'n': 'e',\n",
       "    'r': 'p',\n",
       "    'w': {'cap-color': {'c': 'e',\n",
       "      'g': 'e',\n",
       "      'n': 'e',\n",
       "      'p': 'e',\n",
       "      'w': 'p',\n",
       "      'y': 'p'}}}},\n",
       "  'p': 'p'}}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tree which stopped at depth 3\n",
    "stopping_depth(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "75569181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'odor': {'a': 'e',\n",
       "  'c': 'p',\n",
       "  'f': 'p',\n",
       "  'l': 'e',\n",
       "  'm': 'p',\n",
       "  'n': {'spore-print-color': {'k': 'e',\n",
       "    'n': 'e',\n",
       "    'r': 'p',\n",
       "    'w': {'cap-color': {'c': 'e',\n",
       "      'g': 'e',\n",
       "      'n': 'e',\n",
       "      'p': 'e',\n",
       "      'w': 'p',\n",
       "      'y': 'p'}}}},\n",
       "  'p': 'p'}}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tree which stopped at depth 4\n",
    "stopping_depth(4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a36a56",
   "metadata": {},
   "source": [
    "Implement a test procedure that takes new data and the trained model and returns a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6779a5b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_TP(tree, splitted_df, Ycol_name):\n",
    "    key_list = list(tree.keys())\n",
    "    node = key_list[0]\n",
    "    node_value = splitted_df[node][0]\n",
    "    predict_Y = tree[node][node_value]\n",
    "    actual_Y = splitted_df[Ycol_name][0]\n",
    "    if predict_Y == actual_Y:\n",
    "        return 1\n",
    "    elif type(predict_Y) is not dict:\n",
    "        return 0\n",
    "    else:\n",
    "        return check_TP(predict_Y, splitted_df, Ycol_name)\n",
    "    \n",
    "    \n",
    "def test(tree, test_df, Ycol_name):\n",
    "    TP= 0\n",
    "    for row in test_df.to_numpy():\n",
    "        splitted_df = pd.DataFrame([row])\n",
    "        splitted_df.columns = col_names\n",
    "        #result is 1 when it is TP, 0 otherwise\n",
    "        result = check_TP(tree, splitted_df, Ycol_name)\n",
    "        TP += result\n",
    "    #calculate accuracy percentage\n",
    "    return (TP/(len(test_df))) * 100\n",
    "\n",
    "tree = decision_tree(train_df, Ycol_name)\n",
    "test(tree, test_df, Ycol_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09563ed2",
   "metadata": {},
   "source": [
    "**test evaluation:** I used accuracy to test my decision tree model performance. I calculate the number of correct (TP) values. Based on the number of TP in the testing is used in the accuracy formula: TP/total. The test(tree, test_df, Ycol_name) function results with 100% accuracy, which means my model predicts well with the mushroom data frame. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae8eb07",
   "metadata": {},
   "source": [
    "**************************************** **READ ME** ****************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15aa614b",
   "metadata": {},
   "source": [
    "**-Given a data set-**  \n",
    "First, I read the mushroom data set ,which is called agaricus-lepiota.data, using pandas_read_table. In the data, there are 8124 rows and 23 columns. 23 columns contain 22 features and one for (Y value) edibility.  \n",
    "**Features**: cap-shape, cap-surface, cap-color, bruises, odor, gill-attachment, gill-spacing, gill-size, gill-color, stalk-shape, stalk-root, stalk-surface-above-ring, stalk-surface-below-ring, stalk-color-above-ring, stalk-color-below-ring, veil-type, veil-color, ring-number, ring-type, spore-print-color, population\", habitat   \n",
    "**Label**: edibility  \n",
    "After the data processing, remove rows which contains \"?\" (missing) value, it has 5664 rows.   \n",
    "**edibility**: e(edible), p(poisonous)  \n",
    "  \n",
    "I import the Pandas, NumPy and math libraires. The Pandas library is used in data manipulation, while the NumPy and math libraries are utilized for mathematical calculations or finding unique values and counts. I used for loops to remove missing value \"?\", then the data has 5644 rows.  \n",
    "I divide the dataset into the training and test sets, choose a 70% for training and 30% for test. Used sample() to have random order of the data before spliting.  \n",
    "  \n",
    "**-trains the tree-**  \n",
    "To find our model performance, it is necessary to divide the dataset into training and testing sets, and a 70% (for train_df) and 30% (for test_df) train-validation split is chosen. I used the ID3 algorithm, where at each node, a feature is chosen to split the decision tree into different subsets. The attribute chosen has the highest information gain.  \n",
    "Entropy is used to measure the impurity in a group and is calculated using the formula: H(Y) = -$\\sum_{Y=1}^n p_Y \\times log2 (p_Y)$. A calc_entropy(train_df, col_name) function is used to calculate the entropy of the training data frame (or part of the training data frame).  \n",
    "\n",
    "A calc_IG (train_df, attribute_name, Ycol_name) function is used to measure the information gain of each attribute. Information gain is used to measure the difference between the entropy of the dataset before and after a split, using the formula: Information Gain = H(Y) − H(Y|X), where H(Y|X) = P(X=x) $\\times$ -(P(Y=y|X=x) log2 P(Y=y|X=x)). Then we use a find_highest_IG(train_df, Ycol_name) function to find the attribute which has the highest IG. If there are multiple features with the same highest information gain value, we can choose any of them as the splitting attribute. In this case, I choose the first attribute as the (root) node. When a feature is selected, the decision tree is split into the number of different values that the feature contains.\n",
    "\n",
    "The above steps are repeated to generate the decision tree. After each split, the node feature is removed from the train data frame. The splitting process is continuing until information gain is 0, there are no more attributes to split on, or when the stopping depth is reached. A stopping_depth(depth) function is used to stop the decision tree algorithm when it reaches the given depth. The function uses the majority rule if it is not a pure leaf. You can see the two different decision trees by stopping_depth(2), stopping_depth(3), or stopping_depth(4).  \n",
    "\n",
    "The resulting decision tree is in the form of a dictionary, where the root of the tree is the 'odor' feature and has a depth of 3 (Please see the Task2 A decision tree if you need further information). The value of each key corresponds to the value of the feature, and edibility is at the end of each nested dictionary. A decision_tree(train_df, Ycol_name, tree = None) function is used to generate and print the decision tree. \n",
    "\n",
    "**-applies the tree to data, calculates a performance measure-**  \n",
    "The decision tree can now be used for prediction by recursively traversing the nested dictionary until it reaches the leaf node where the edibility is found or it is no more dictionary type. The test_df(30% of cleaned data frame) is used in a test(tree, test_df, Ycol_name) function to find the accuracy, using the formula: TP/total(TP+FP). To calculate the number of true positive values, the check_TP(tree, splitted_df, Ycol_name) function is used. The function compares the prediction and actual value. If two values are the same, it returns 1 so we can increase the true positive count. Otherwise, it returns 0. The result of testing indicates that our training model has 100% accuracy with the test data. However, if we change the data set, we cannot guarantee the accuracy percentage would be 100% also.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2efbdac1",
   "metadata": {},
   "source": [
    "** The evaluation method can indicate whether your tree is over- or underfitting? **\n",
    "\n",
    "Overfitting is when a model has 100% accuracy with the train data while it has poor accuracy with the test data. This indicates that our model did not capture the relationship which are in test data. In this case, we can prune the tree to make it more general. On the other hand, underfitting is when a model has poor accuracy with both the train and test data. This indicates that our model fails to capture relationship which are in both data sets. In this case, we can increase the depth of the tree in order to increase the accuracy of both data sets. When my training decision tree only has depth 2, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f002f409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99.88186650915534"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#accuracy when tree depth is 2\n",
    "test(stopping_depth(2), test_df, Ycol_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7c959dfb",
   "metadata": {},
   "source": [
    "it has a 99.9% accuracy with the test data.  \n",
    "This indicates that my evaluation method to generate a tree is not underfitting and perfectly fits. However, in general, if a trained model has 100% accuracy with the train data, it would have lower accuracy since the tree is overfitting. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "edc5e595d7ef337b338ddd9c3b7d927795bc30a52fb6b7c2dd8276fdae2f0a37"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
